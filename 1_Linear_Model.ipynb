{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVaDYtmoZogDYzk6ShA0F9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrSubbiah/LinearModels/blob/main/1_Linear_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# <font color = maroon>Introduction\n",
        "\n",
        "This notes provides the necessary mathematical and statistical details of Linear Modeling (LM). \n",
        "\n",
        "**<font color = blue>Suggested Reading: [MPG] Introduction to Linear Regression Analysis, Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining**\n",
        "\n",
        "**<font color = blue>Suggested Reading: [AAG] Foundations of Linear and Generalized Linear Models Wiley Series in Probability and Statistics Alan Agresti**\n",
        "\n",
        "## <font color = blue>Objectives\n",
        "\n",
        "1. To understand the need for building relational models between response and a set of predictors, conceptualized from the contextual mapping. \n",
        "\n",
        "1. To know the difference between Linear and Non-Linear models\n",
        "\n",
        "1. To **interpret** the estimated **weights** in a model\n",
        "\n",
        "1. To understand the meaning <font color =  darkred>**Confidence Interval (CI)** for concluding <font color = darkred>**statistical significance** of a predictor\n",
        "\n",
        "## <font color = blue>Points to Ponder\n",
        "\n",
        "1. Major aim is to understand ways to build a mathematical relation / mapping from a contextual possible relationships between quantities involved in a real time situation\n",
        "\n",
        "1. It is possible to construct different forms of such mapping in an explicit mathematical form \n",
        "\n",
        "1. Liner Model (LM) - A straightline (Linear) mapping is an easiest way to build mappings / models. However, it is possible to go beyond such modeling to construct Non-Linear (Curvilinear) models\n",
        "\n",
        "1. Symbollically, such  models are represented as $Y=f(X)$ where $Y$ is the response variable and $X$ is a set of predictors identified through a contextual mapping from a study / data. $f$ is the relation / mapping /  function between $X$ and $Y$. Generally, such practices are termed as Modeling\n",
        "\n",
        "1. There are two major types of modeling, depends majorly on the objective of the problem at hand; either to develop model  to **understand (quantify)** the possible relationship between $x$ and $Y$ or the interest is **only to predict** value of $Y$ for a given $X$. \n",
        "\n",
        "1. This notes confines to the first modeling practice - interpretable or explainable model. However, a minor point is made related to the predictive models with regard to a high level understanding of the process of estimation and the way error term is defined\n",
        "\n",
        "1. In real time situations (predominantly random, unpredictable events), this mapping includes a component of error / noise in the data; the actual model is $Y=f(X)+\\textrm{Error}(\\epsilon)$. \n",
        "\n",
        "1. If we assume, $k$ is the number of predictors then the Linear form of $f$ is \n",
        "$$Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\cdot\\cdot\\cdot\\cdot+\\beta_kX_k+\\epsilon$$\n",
        "$\\beta's$ are **weights / coefficients / betas** that are to be **estimated**\n",
        "Errors $(\\epsilon)$ are quite random we **estimate**  and not try to **determine** the coefficients $\\beta s$\n",
        "\n",
        "1. Methods to build LM (and other models) is **highly dependent on the nature of the response variable**. Additionally, some models will have few assumptions in constructing such models. Here, we assume the response variable as numeric (measurable, continuous) one\n",
        "\n",
        "\n",
        "\n",
        "#### Necessary Symbols:\n",
        "\n",
        "1. k: Number of predictors\n",
        "1. p = k + 1\n",
        "1. X: Model Matrix of predictors including constant term  \n",
        "$$X = \\begin{pmatrix}\n",
        "  x_{ij} \\end{pmatrix}_{n\\times p}$$ such that $x_{i1} = 1 ~~~~~~\\forall i = 1,2\\cdots n$\n",
        " \n",
        "\n",
        "\n",
        "1. Y: Response variable \n",
        "$$Y =\n",
        " \\begin{pmatrix}\n",
        "  y_{1} & y_{2} & \\cdots & y_{n} \n",
        " \\end{pmatrix}^T_{n\\times 1}$$\n",
        "\n",
        "1. Mean Response $\\mu = E[Y]$\n",
        "$$\\mu =\n",
        " \\begin{pmatrix}\n",
        "  \\mu_{1} & \\mu_{2} & \\cdots & \\mu_{n} \n",
        " \\end{pmatrix}^T_{n\\times 1}$$ with $\\mu_i=E[y_i]$\n",
        " \n",
        "1. $\\beta$: Parameter matrix \n",
        " $$\\beta =\n",
        " \\begin{pmatrix}\n",
        "  \\beta_{1} & \\beta_{2} &   \\cdots  & \\beta_{p} \n",
        " \\end{pmatrix}^T_{p\\times 1}$$ \n",
        " \n",
        "1. $\\epsilon$: Random error matrix. The errors are assumed to have mean zero and unknown variance $\\sigma^2$; also, it is assumed that the errors are uncorrelated.\n",
        "\n",
        " $$\\epsilon =\n",
        " \\begin{pmatrix}\n",
        "  \\epsilon_{1} &  \\epsilon_{2} &  \\cdots & \\epsilon_{n} \n",
        " \\end{pmatrix}^T_{n\\times 1}$$ \n",
        " \n",
        "A Linear Model is $Y=X\\beta + \\epsilon$. If the predictors are controlled by the researchers in that X is measured or observed with negligible error and Y is a random variable then,\n",
        "\n",
        "$$E(y|x)=\\sum \\beta_jx_{ij}$$ and $$V(y|x)=\\sigma^2$$. Mean of y is a linear function of y. Since the errors are uncorrelated, the responses are also uncorrelated. \n",
        "\n",
        "## <font color = darkgreen> Aim 1: Parameter estimation ($\\hat \\beta$) and finding fitted values ($\\hat \\mu$)\n",
        "\n",
        "The notion of Least Square Method (LSM) is to minimize the value of $||Y-\\hat \\mu||^2$\n",
        "\n",
        "\n",
        "$$||Y-\\hat \\mu||^2 = \\sum_{i=1}^{n}(y_i-\\hat\\mu_i)^2= \\sum_{i=1}^{n}(y_i-\\sum_{j=1}^{p}\\hat\\beta_jx_{ij})^2$$\n",
        "Similar expression can be obtained from the notion of MLE when we assume that $\\forall i = 1,2,\\cdots n~~~~ y_i\\sim \\text{Normal}(\\mu_i,\\sigma^2)$. \n",
        "\n",
        "$\\Rightarrow$ the log likelihood (excluding constant) is $$l(\\beta|y) = -\\frac{\\sum_i (y_i-\\mu_i)^2}{2\\sigma^2}$$. Hence, maximizing likelihood is equivalent to minimize the $\\sum_i (y_i-\\mu_i)^2$\n",
        "\n",
        "Hence $l(\\beta) = \\sum_{i=1}^{n}(y_i-\\sum_{j=1}^{p}\\beta_jx_{ij})^2$ yields p equations while we equate $\\frac{\\partial~ l }{\\partial~ \\beta_j}$ to zero $\\forall j=1,2,\\cdots p$\n",
        "\n",
        "That is $\\sum_i(y_i-\\mu_i)x_{ij}=0 ~~~~~\\forall j=1,2,\\cdots p$\n",
        "\n",
        "Hence the LS estimates satisfy $\\sum_iy_ix_{ij}=\\sum_i\\hat\\mu_ix_{ij}~~~~~\\forall j=1,2,\\cdots p$\n",
        "\n",
        "#### Matrix Notation \n",
        "\n",
        "$l(\\beta)=||y-X\\beta||^2=(y-X\\beta)^T(y-X\\beta)$\n",
        "\n",
        "=$(y^T-\\beta^TX^T)(y-X\\beta)$\n",
        "\n",
        "=$y^Ty-y^TX\\beta-\\beta^TX^Ty+\\beta^TX^TX\\beta$. It can be observed that $y^TX\\beta$ is a constant and hence $\\beta^TX^Ty=(y^TX\\beta)^T=y^TX\\beta$\n",
        "\n",
        "Hence, $l(\\beta)=y^Ty-2y^TX\\beta+\\beta^TX^TX\\beta$\n",
        "\n",
        "#### Results: $\\frac{\\partial (a^T\\beta)}{\\partial\\beta}=a$ and $\\frac{\\partial (\\beta^TA\\beta)}{\\partial\\beta}=(A+A^T)\\beta=2A\\beta ~~~~~\\text{(if A is symmetric)}$\n",
        "\n",
        "Using these results $\\frac{\\partial ~l}{\\partial\\beta}=-2X^Ty+2X^TX\\beta=-2X^T(y-X\\beta)$\n",
        "\n",
        "Hence, $\\frac{\\partial ~l}{\\partial\\beta}=0 \\Rightarrow X^TX\\beta=X^Ty$\n",
        "\n",
        "$\\Rightarrow \\hat\\beta= (X^TX)^{-1}X^Ty$\n",
        "\n",
        "Also, $\\frac{\\partial^2 ~l}{\\partial\\beta^2}=2X^TX$ which is positive definite and hence the minimum exists at $\\hat\\beta$\n",
        "\n",
        "Let us define hat matrix H as $H=X(X^TX)^{-1}X^T$. The fitted values are $\\hat\\mu=X\\hat\\beta=Hy$\n",
        "\n",
        "Also, $E(\\hat\\beta)=\\beta$ and $V(\\hat\\beta)=\\sigma^2(X^TX)^{-1}$. These results are obtained from the expectation of matrices \n",
        "\n",
        "1. $E(Ay)=AE(y)$\n",
        "\n",
        "2. $V(Ay)=AV(y)A^T$\n",
        "\n",
        "The estimate of $\\sigma^2$ is $s^2=\\frac{\\sum(y_i-\\hat\\mu_i)^2}{n-p}$. It can be noted that $V(\\hat\\beta)$ is the main diagonal elements of the matrix $\\hat{\\sigma^2}(X^TX)^{-1}$. \n",
        "\n",
        "Using the point estimates of $\\beta$ and associated $V(\\hat\\beta)$ it is possible to obtain 100(1-$\\alpha$)% **confidence intervals** for $\\hat\\beta$.\n",
        "\n",
        "## <font color = darkgreen> Aim 2: Interpretation of $\\beta$\n",
        "\n",
        "### <font color=\"blue\"> Numeric predictors\n",
        "\n",
        "If the fitted LM has a numeric predictor $X$, then the corresponding weight $\\beta$ is interpreted as the **change ** in the average value of the response variable $Y$ when the predictor $X$ is increased by one unit, provided other predictors are kept in a same value (constant); constant may be zero or any appropriate value\n",
        "\n",
        "### <font color=\"blue\">Categorical predictors\n",
        "\n",
        "Following are the steps to interpret factor predictors\n",
        "\n",
        "1. One of the levels will be considered Reference or Base level\n",
        "\n",
        "    - this would be software specific; however, options may be provided to change / fix base level\n",
        "    \n",
        "1. Weights $(\\beta)$ are the **changes in the mean** response $Y$ when the levels of factor $X$ are compared with base level\n",
        "\n",
        "1. Other predictors are kept constant; factor predictors, are kept in the **same level**\n",
        "\n",
        "### <font color=\"blue\">Intercept or Constant\n",
        "\n",
        "Constant in the model provides the average response when all the predictors are kept at the same level   \n",
        "\n",
        "\n",
        "\n",
        "## <font color = darkgreen> Aim 3: Significance of Predictors $\\beta$\n",
        "\n",
        "Once a linear model is built and interpretation of the weights are appropriately done, then we have a step to check the **Significance** of the predictors. This is carried out using the estimated weights.\n",
        "\n",
        "<font color =  maroon> Meaning of Significance\n",
        "\n",
        "- **Statistical:** We expect that effect of a predictor is **not** mere due to chance but there may be a _real_ effect of predictor in explaining the variation in the response\n",
        "\n",
        "- **Practical:** On the other hand, from the context point of view one (an expert) may assess that a predictor is having impact on the relational mapping, irrespective of _statistical_ significance\n",
        "\n",
        "- **Statistical Significance** can be studied using two approaches \n",
        "\n",
        "1. <font color =  darkgreen> Testing of Significance:\n",
        "    -  Set a hypothesis of $\\beta=0$ \n",
        "    -  Test its significance using an appropriate test statistic\n",
        "    -  Rejection of the hypothesis implies the significance (statistical) of the respective predictor\n",
        "\n",
        "1. <font color =  darkgreen> Confidence Interval (CI) \n",
        "\n",
        "\n",
        "- CI is constructed with a pre specified level of significance (before conducting the experiment), say 5%, a kind of margin of error\n",
        "\n",
        "- CI covers a set of plausible values for the parameter about the population of interest\n",
        "\n",
        "- For each $\\beta$, form of CI is a real interval (L, U) where L < U\n",
        "    \n",
        "    - L: Lower Limit, U: Upper Limit  \n",
        "    \n",
        "    - There are uncountable infinite values in between L and U\n",
        "    \n",
        "    - If a CI **covers zero** (L is negative, U is positive), corresponding predictor is <font color = maroon>**not statistically significant**\n",
        "\n",
        "    - If a CI **does not cover zero**, corresponding predictor is  <font color=darkgreen> **statistically significant**.\n",
        "    \n",
        "    In this case, LL and UL have same sign (either negative or positive)\n",
        "\n",
        "\n",
        "To obtain the test statistic, let us define the following \n",
        "\n",
        "1. Total sum of squares TSS = $\\sum(y_i-\\bar y)^2$\n",
        "2. Error sum of squares SSE = $\\sum(y_i-\\hat \\mu_i)^2$\n",
        "3. Regression sum of squares SSR = $\\sum(\\hat \\mu_i-\\bar y)^2$\n",
        "\n",
        "**Following results from these three quantities are important**\n",
        "\n",
        "TSS = SSE + SSR\n",
        "\n",
        "$\\frac{\\text{SSE}}{\\sigma^2}\\sim \\chi^2(n-p)$\n",
        "\n",
        "$\\frac{\\text{SSR}}{\\sigma^2}\\sim \\chi^2(p-1)$\n",
        "\n",
        "It can be noted that Error Mean square  MSE = $\\frac{\\text{SSE}}{n-p}$ \n",
        "\n",
        "Regression Mean square  MSR = $\\frac{\\text{SSR}}{p-1}$ \n",
        "\n",
        "$\\text{F-Ratio}=\\frac {MSR}{MSE} \\sim \\text{F}_{p-1,n-p}$\n",
        "\n",
        "Using TSS,SSE,and SSR it is possible to perform the global test for **statistical significance** of regression; \n",
        "\n",
        "$$\\text{H}_0:\\beta_1=\\beta_2=\\beta_3=\\cdots\\cdots\\beta_k=0 ~~\\text{vs}~~ \\text{H}_1: \\beta_j\\ne 0$$ at least for one $j=1,2,\\cdots ~k$\n",
        "\n",
        "If $\\text{F}_0$ is calculated F- ratio  then reject $\\text{H}_0$ if  $\\text{F}_0>\\text{F}_{p-1,n-p}$\n",
        "\n",
        "In a similar way, a test on individual regression coefficients can be performed for their **statistical significance** \n",
        "\n",
        "That is test $$\\text{H}_0:\\beta_j= 0 ~~\\text{vs}~~ \\text{H}_1: \\beta_j\\ne 0 ~~~ \\forall j = 1,2,\\cdots~p$$ \n",
        "\n",
        "Reject $\\text{t}_0$ if  $\\text{t}_0>\\text{t}_{\\alpha/2,n-p}$ where $\\text{t}_0=\\frac{\\hat\\beta_j}{\\sqrt{(V(\\hat\\beta_j))}}$\n",
        "\n",
        "## <font color = darkgreen>Aim 4: Model Diagnostics\n",
        "\n",
        "### <font color = darkblue>Terms and Terminologies\n",
        "\n",
        "1. Data = fit + residuals; $Y =  X \\beta +\\epsilon$ Fitted model is $\\hat Y =  X \\hat\\beta$ and * The difference between the observed value $y_i$ and the fitted value $\\hat y_i$ is a residual $\\epsilon_i= y_i-\\hat y_i$\n",
        "    \n",
        "1. Standardized residual (or internally studentized) divides each raw residual by its standard error (For an observation $i$)\n",
        "\n",
        "1. Studentized residual is based on the fit of the model after excluding $i^{th}$ observation (For an observation $i$)\n",
        "\n",
        "1. Leverage determines the precision of estimating $\\mu$. Larger the values $y_i$ may have a large influence on $\\hat \\mu_i$. When the leverage is relatively large, $y_i$ is highly correlated with $\\hat \\mu_i$. \n",
        "\n",
        "1. Larger leverage with high residual may be influential observation. \n",
        "\n",
        "1. Akaike and Bayesian Information Criterions (AIC and BIC) for comparing models. The competing models need not be nested or based on the same family of distributions for the random component\n",
        "\n",
        "\n",
        "## <font color = darkblue>Residuals\n",
        "\n",
        "1.  Raw or response residuals are the difference between the observed and the fitted value $y_i-\\hat y_i$\n",
        "\n",
        "1.  Pearson residuals are raw residuals divided by the standard error of the observed value se($y_i$)\n",
        "\n",
        "1.  Standardized Pearson residuals are the raw residuals divided by their standard error \n",
        "se($y_i-\\hat y_i$)\n",
        "\n",
        "$r_i=\\frac{y_i-\\hat\\mu_i}{s\\sqrt{1-h_{ii}}}$ where $\\text{h}_{ii}$ is the $\\text{i}^{th}$ main diagonal element of the hat matrix H; $\\text{s}^2$ is the estimator of $\\sigma^2=\\text{V}(y_i)$ \n",
        "\n",
        "If the linear model holds, then $-3\\le r_i \\le 3$\n",
        "\n",
        "#### $\\textrm{Cook's distance}_{i}$\n",
        "\n",
        "For observation $i$, $D_i$ combines the magnitude of residual and the location in X space to understand the influence.It is based on the change in $\\hat\\beta$ when observation $i$ is removed from the data set. \n",
        "\n",
        "$\\text{D}_i=\\frac{h_{ii}~(y_i-\\hat\\mu_i)^2}{ps^2(1-h_{ii})^2}$ \n",
        "\n",
        "$D_i$ more than 1 may require attention. Usually larger D ($\\text{D}_i\\ge1$), occurs when both Standardized Residual $r_i$ and the leverage are relatively large\n",
        "\n",
        "## <font color = darkblue> Comparing Models\n",
        "\n",
        "Sometimes we might include irrelevant predictors; we use model selection criterions (minimizing AIC or BIC). AIC can aid in variable selection\n",
        "\n",
        "AIC =$-2\\Big[l(\\hat\\beta)- p_1\\Big]$ where $p_1$ is the number of parameters in the model. In the case of linear model this includes the constant variance $\\sigma^2$\n",
        "\n",
        "For linear model, $l(\\hat\\beta)=-\\frac{n}{2}~\\log(s^22\\pi)-\\frac{1}{2s^2}\\sum(y_i-\\mu_i)^2$\n",
        "\n",
        "#### $\\text{DFBETAS}_{j,i}$\n",
        "\n",
        "* Measures both leverage and the effect of large residual. If the absoluate value of $\\textrm{|DFBETAS}_{j,i}|$ exceeds $\\frac{2}{\\sqrt n}$ then the $i^{th}$ observation may require an attention\n",
        "\n",
        "#### $\\text{DFFITS}_{i}$\n",
        "\n",
        "* Measures the effect of the deletion influence of $i^{th}$ observation on the fitted or predicted value. \n",
        "\n",
        "* If the absoluate value of $\\textrm{|DFFITS}_{i}|$ exceeds $\\frac{2}{\\sqrt{p/n}}$ then the $i^{th}$ observation may require an attention\n",
        "\n",
        "#### $\\text{Covariance ratio}_{i}$\n",
        "\n",
        "Observations with |COVRATIO-1| near to or larger than  $\\frac{3p}{n}$ may require an attention"
      ],
      "metadata": {
        "id": "KPYYKlBI5sPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = maroon> Variance Components\n",
        "\n",
        "## <font color = darkgreen> Recall: Parameter estimation ($\\hat \\beta$) \n",
        "\n",
        "We minimize the sum of squares of residuals; where the ***Residual (e)*** is given by $e=y-\\hat y$;it could be observed that, $e=y-\\hat y = Y - X\\hat\\beta$ which is a $n\\times1$ matrix\n",
        "\n",
        "In matrix notation, the sum of squared residuals is $$e^Te = (Y-X\\hat\\beta)^T(Y-X\\hat\\beta)$$ $$=Y^TY-2\\hat\\beta^TX^TY+\\hat\\beta^TX^TX\\hat\\beta$$\n",
        "\n",
        "If $X$ is a full rank matrix and $X^TX$ is a positive definite matrix then **<font color = darkgreen> $\\hat\\beta=(X^TX)^{-1}X^TY$**\n",
        "\n",
        "\n",
        "### <font color = maroon> Gauss Markov Assumptions\n",
        "\n",
        "1. Linearity $Y=X\\beta+\\epsilon$ \n",
        "\n",
        "1. $X$ is a full rank matrix\n",
        "\n",
        "1. $E[\\epsilon|X] = 0$; given any $X$, no information of predictors convey any information about errors\n",
        "\n",
        "1. $E[\\epsilon\\epsilon^T|X] = \\sigma^2I$ where $I$ is the identity matrix of order $n$; this helps to handle constant variance and no autocorrelation between the errors\n",
        "\n",
        "1. $\\epsilon \\sim N(0,\\sigma^2I)$, largely helpful for inference about $\\hat\\beta$\n",
        "\n",
        "**Form of $E[\\epsilon\\epsilon^T|X]$**\n",
        "\n",
        "$$E[\\epsilon\\epsilon^T|X] =\n",
        " \\begin{pmatrix}\n",
        "  V(\\epsilon_{1}) & COV(\\epsilon_{1}\\epsilon_{2}) &   \\cdots  & COV(\\epsilon_{1}\\epsilon_{n})\\\\\n",
        "   COV(\\epsilon_{2}\\epsilon_{1}) & V(\\epsilon_{2}) &   \\cdots  & COV(\\epsilon_{2}\\epsilon_{n})\\\\\n",
        "   \\vdots & \\vdots & \\ddots &\\vdots\\\\   COV(\\epsilon_{n}\\epsilon_{1}) & COV(\\epsilon_{n}\\epsilon_{2})&   \\cdots  & V(\\epsilon_{n}) \n",
        " \\end{pmatrix}_{n\\times n}$$ \n",
        "\n",
        "If the assumptions are constant variance and zero auto correlation then the above matrix is $diag(\\sigma^2)_{n\\times n} = \\sigma^2I_{n\\times n}$\n",
        " \n",
        "<font color = blue> In general let $E[\\epsilon\\epsilon^T|X]=\\Sigma$\n",
        "\n",
        "### <font color = darkgreen> Note on $\\hat \\beta$\n",
        "\n",
        "1. $\\hat\\beta=(X^TX)^{-1}X^TY=(X^TX)^{-1}X^T(X\\beta+\\epsilon)$ which may be reduced to $\\hat\\beta=\\beta+(X^TX)^{-1}X^T\\epsilon$\n",
        "\n",
        "1. This indicates $\\hat\\beta$ is **linear in $\\epsilon$**\n",
        "\n",
        "1. Also to note $E[\\hat\\beta]=\\beta$ (Unbiased)\n",
        "\n",
        "1. It can be proved that $\\hat\\beta$ has minimal variance among all linear unbiased estimate\n",
        "\n",
        "1. $COV(\\hat\\beta) = E[(\\hat\\beta-\\beta)(\\hat\\beta-\\beta)^T]$=$(X^TX)^{-1}X^TE[\\epsilon\\epsilon^T]X(X^TX)^{-1}$=$(X^TX)^{-1}X^T\\Sigma X(X^TX)^{-1}$which is a $p\\times p$ matrix\n",
        "\n",
        "1. Under constant variance and zero auto correlation this becomes $COV(\\hat\\beta)=\\sigma^2(X^TX)^{-1}$\n",
        "\n",
        "1. Hence, with normality assumption on errors, we have $$\\hat\\beta\\sim N(\\beta, \\sigma^2(X^TX)^{-1})$$\n",
        "\n",
        "1. Also the estimate of $\\sigma^2$ is $\\frac{e^Te}{n-p}$\n",
        "\n",
        "1. **With this unbiased estimate of $\\beta$,\n",
        "MSE($\\hat\\beta$)=V($\\hat\\beta$)**\n",
        "\n",
        "### <font color = darkgreen> Notion of Shrinkage\n",
        "\n",
        "**<font color = red> Reduce variance with biased estimators**\n",
        "\n",
        "Recall our objective (minimization) function $$e^Te =\\sum_{1\\le i\\le n}\\big(y_i-\\sum_{1\\le j\\le p}\\beta_j x_{ij}\\big)^2$$\n",
        "\n",
        "Consider a slightly modified form of this objective function to reduce the variance at the cost of having bias in the estimation of $\\beta$\n",
        "\n",
        "$$J(\\beta)=[e^Te+\\lambda ||\\beta||]$$ where $||\\beta||$ is suitable norm of $\\beta$\n",
        "\n",
        "**<font color = blue> $L_2$ NORM**\n",
        "\n",
        "In case of $L_2$ norm this becomes $$J(\\beta)=\\frac{1}{2}[e^Te+\\lambda\\sum_{1\\le j\\le p}\\beta_j^2 ]$$\n",
        "\n",
        "Hence the original aim of minimizing sum of squares becomes minimizing $J(\\beta)$. A closed form solution is \n",
        "\n",
        "$$\\hat\\beta = (X^TX+\\lambda I)^{-1}X^TY$$\n",
        "\n",
        "**<font color = darkred> Let us utilise eigen decomposition of $X^TX$ to proceed further in understanding variance components**\n",
        "\n",
        "Let $X^TX = UDU^T$ where columns of $U$ are the eigen vectors of $X^TX$ and $D = diag(\\delta_1,\\delta_2,\\cdots \\delta_p)$ is a diagonal matrix of eigen values of  $X^TX$\n",
        "\n",
        "Hence, $X^TX+\\lambda I=Udiag(\\delta_j+\\lambda)U^T$ and its inverse is $(X^TX+\\lambda I)^{-1} = Udiag(\\frac{1}{\\delta_j+\\lambda})U^T$\n",
        "\n",
        "As before, $$\\hat\\beta=(X^TX+\\lambda I)^{-1}X^T(X\\beta+\\epsilon)$$\n",
        "$$=(X^TX+\\lambda I)^{-1}X^TX\\beta+(X^TX+\\lambda I)^{-1}X^T\\epsilon$$\n",
        "\n",
        "And the expectation is \n",
        "$$E[\\hat\\beta]=(X^TX+\\lambda I)^{-1}X^TX\\beta$$\n",
        "$$=Udiag(\\frac{1}{\\delta_j+\\lambda})U^TX^TX\\beta$$\n",
        "\n",
        "$$=Udiag(\\frac{1}{\\delta_j+\\lambda})U^TUdiag(\\delta_j)U^T\\beta$$\n",
        "\n",
        "$$=Udiag(\\frac{\\delta_j}{\\delta_j+\\lambda})U^T\\beta$$\n",
        "\n",
        "Also, if $Cov(\\epsilon)=\\sigma^2I$ then $Cov(\\hat\\beta)=Udiag(\\frac{\\sigma^2\\delta_j}{\\delta_j+\\lambda})U^T\\beta$\n",
        "\n",
        "From these relations, we can see Bias - Variance tradeoff is a function of $\\lambda$. \n",
        "\n",
        "\n",
        "**<font color = blue> $L_1$ NORM**\n",
        "\n",
        "In case of $L_1$ norm  $$J(\\beta)=e^Te+\\lambda\\sum_{1\\le j\\le p}|\\beta_j|$$\n",
        "This is called ***LASSO*** - least absolute shrinkage and selection operator. Unlike, ***RIDGE REGRESSION*** ($L_2$ norm) LASSO does not have closed form solution to get $\\hat\\beta$. The major question in such models is **<font color = red> How do we fix $\\lambda$**\n",
        "\n",
        "### <font color = darkgreen> A word about Predictive Models\n",
        "\n",
        "This statistical input will guide the bias-variance trade-off in predictive models (irrespective of parameteric or non-parametric). In both settings, aim is to ***estimate*** a function $f$ that can predict y given x\n",
        "\n",
        "In practice, we have training data to ***estimate*** $f$ that ***\"predicts well\"*** in unseen data\n",
        "\n",
        "As before the assumption is $y=f(x)+\\epsilon$ with $E(\\epsilon)=0$ and $V(\\epsilon)=\\sigma^2$ and $\\epsilon$ ***not necessarily*** $\\sim N(0,\\sigma^2)$. It should also be noted that we define $f(x) = E[y|X=x]$ as ***\"true\"*** $f$\n",
        "\n",
        "With all these observations, we shall find the MSE of $\\hat f$ which is similar to statistical learning but ***noise in the test data*** will generate some variance which is irreducible in nature. Following mathematics will provide more clear and clean picture\n",
        "\n",
        "$$MSE(\\hat f)=E[y-\\hat f]^2$$\n",
        "$$=E[f+\\epsilon-\\hat f]^2$$\n",
        "$$=E[(f-\\hat f)+\\epsilon]^2$$\n",
        "$$=E[(f-\\hat f)^2]+E[\\epsilon^2]+2E(\\epsilon)(f-\\hat f)$$\n",
        "$$=E[(f-\\hat f)^2]+E[\\epsilon^2]$$ $(\\because E[\\epsilon]=0)$\n",
        "\n",
        "$$=V[f-\\hat f]+E[(f-\\hat f)]^2+V[\\epsilon]$$ $(\\because E(X^2)=V(X)+E[X]^2)$ \n",
        "\n",
        "$$=V[\\hat f]+E[(f-\\hat f)]^2+V[\\epsilon]$$ $(\\because E(X^2)=V(X)+E[X]^2$ and $V[X+a]=V(X))$\n",
        "\n",
        "\n",
        "<font color = blue> ***Final Result***\n",
        "\n",
        "<font color = blue> $MSE(\\hat f)=V(\\hat f)+Bias(\\hat f)^2+V(\\epsilon)$ and **<font color = red> $V(\\epsilon)$:Irreducible Error** \n",
        "\n",
        "## Final Remarks\n",
        "\n",
        "Following steps may be followed to build a LM, interpret and finding the significance (statistical) of weights.\n",
        "\n",
        "1. Contextual mapping - Data context - Data domain\n",
        "\n",
        "2. Fitting model - use software\n",
        "\n",
        "    - Scaling the numeric predictors\n",
        "\n",
        "    - Estimate the weights / betas / coefficients\n",
        "\n",
        "3. Interpret the weights\n",
        "\n",
        "4. Estimate the limits of Confidence Interval \n",
        "\n",
        "5. Interpret CI\n",
        "\n",
        "6. Use model diagnostics principles to choose between competing models, if any"
      ],
      "metadata": {
        "id": "RZLG0c4XBs7s"
      }
    }
  ]
}