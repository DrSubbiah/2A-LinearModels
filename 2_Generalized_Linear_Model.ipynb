{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6pNr525y/ftXGK2nVAANk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrSubbiah/LinearModels/blob/main/2_Generalized_Linear_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# <font color = maroon>Introduction\n",
        "\n",
        "This notes provides the necessary mathematical, statistical and computational details of Generalized Linear Model (GLM). \n",
        "\n",
        "## <font color = darkgreen> Three components of GLM \n",
        "  \n",
        "1. <font color = red>Random Component: Response variable $Y$ with independent observations $(y_1,., y_n)$ having probability density or mass function for a distribution \n",
        "\n",
        "2. <font color = blue>A linear predictor: For observation $i~~ (i = 1,.,n)$, let $x_{ij}$ denote the value of explanatory variable  $x_j~~ (j = 1,....,p)$. Let $x_i$ = ($x_{i1}$,.,$x_{ip}$). Usually, we set $x_{i1}$ = 1, so that it serves as the coefficient of the intercept term in a model. \n",
        "\n",
        "3. <font color = purple>A link function: Relates parameter $\\eta_i$ related to E($y_i$) to the explaintory variables $x_1$,$x_2$,$x_3$,....,$x_p$ using a linear combination $\\eta_i = \\sum\\beta_jX_{ij},~~~~~~j=1,2,3,...,p,~~~~~~ i = 1,.,n.$\n",
        "\n",
        "First let us confine to two random components (Bernoulli and Poisson) from an exponential family of distribution and canonical link functions.\n",
        "\n",
        "## <font color = darkgreen> Expontential Families\n",
        "\n",
        "Expontential family of distribution has likelihood of the form\n",
        "\n",
        "$f(y|\\theta,\\phi) = \\exp(\\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y,\\phi))$ where $\\theta$ is natural parameter and $\\phi$ is scale parameter\n",
        "\n",
        "## <font color = darkgreen> Preliminary Results\n",
        "\n",
        "### <font color = darkblue> Result 1:$E[\\frac{\\partial L}{\\partial \\theta }] = 0$\n",
        "\n",
        "### <font color = darkblue> Result 2:$E[\\frac{\\partial^{2} l}{\\partial \\theta^{2} }] = -E[(\\frac{\\partial l}{\\partial \\theta })^{2}]$\n",
        "\n",
        "**Proofs of these two results are available at the end of this notes**\n",
        "\n",
        "From Result 1, it can be observed that for an exponential family of distribution \n",
        "\n",
        "$E[Y] = \\frac{\\partial l}{\\partial \\theta} = b^{'}(\\theta)$\n",
        "\n",
        "Also  following is an immediate consequence of Result 2 \n",
        "\n",
        "$E[\\frac{\\partial^2 l}{\\partial \\theta^2}] = -\\frac{b^{''}(\\theta)}{a(\\phi)}$\n",
        "\n",
        "Hence, $\\frac{b^{''}(\\theta)}{a(\\phi)} = \\frac{V(Y)}{a(\\phi)^2}$\n",
        "$\\Rightarrow V(Y) = a(\\phi)b^{''}(\\theta)$\n",
        "\n",
        "\n",
        "## <font color = darkgreen> Canonical link\n",
        "If  $\\eta= \\theta$, then the link function $\\eta$ is said to be canonical link \n",
        "\n",
        "### <font color = darkblue>Example 1: Bernoulli Distribution\n",
        "\n",
        "$Y_{1},Y_{2},Y_{3},....,Y_{n} \\sim \\text{Bernoulli}(p_{i})$\n",
        "\n",
        "$L(p_i|y) = \\prod p_i^{y_i} (1-p_i)^{1-y_i}$ = $= \\prod \\exp\\Big[y_i\\log p_i + (1-y_i) \\log (1-p_i)\\Big]$\n",
        "\n",
        "$= \\exp\\sum\\Big[y_i\\log p_i + (1-y_i) \\log (1-p_i)\\Big]$\n",
        "\n",
        "$= \\exp\\sum\\Big[y_i\\log \\frac{p_i}{1-p_i} + \\log (1-p_i)\\Big]$\n",
        "\n",
        "Hence, $l(\\theta |y) = \\sum\\Big[y_i\\theta_i - \\log (1+\\exp~ \\theta_i)\\Big]$ where $\\theta_i = \\log \\frac{p_i}{1-p_i}$\n",
        "\n",
        "$\\theta_i = \\log \\frac{p_i}{1-p_i}; b(\\theta_i) = \\log(1+\\exp~\\theta_i);~ \\text{and}~ a(\\phi) =1$\n",
        "\n",
        "$\\Rightarrow b^{'}(\\theta_i) = \\frac{e^{\\theta_i}}{1+e^{\\theta_i}} = p_i$ and \n",
        "\n",
        "$~b^{''}(\\theta_i) = \\frac{e^{\\theta_i}}{(1+e^{\\theta_i})^2}=p_i(1-p_i)$\n",
        "   \n",
        "\n",
        "### <font color = darkblue>Example 2: Poisson Distribution\n",
        "\n",
        "$Y_{1},Y_{2},Y_{3},....,Y_{n} \\sim \\text{Poisson}(\\lambda_{i})$\n",
        "\n",
        "$l(\\lambda_{i}|y) = \\prod e^{-\\lambda_{i}}\\frac{\\lambda_{i}^{y_{i}}}{y_{i}!}  = \\prod \\exp[-\\lambda_{i} \\log(\\frac{\\lambda_{i}^{y_{i}}}{y_{i}!})]$\n",
        "\n",
        "$= \\exp(\\sum^{n}_{i=1} ( -\\lambda_{i}  + y_{i}\\log\\lambda_{i} - \\log y_{i}!))$\n",
        "\n",
        "$= \\exp(\\sum^{n}_{i=1} (\\frac{y_{i}~\\log\\lambda_{i}-\\lambda_{i}}{1}  - \\log y_{i}!))$\n",
        "\n",
        "\n",
        "$\\Rightarrow l(\\theta|y) =  \\sum^{n}_{i=1} (\\frac{y_{i}~\\log\\lambda_{i}-\\lambda_{i}}{1}  - \\log y_{i}!)$\n",
        "\n",
        "\n",
        "$\\theta_{i} = \\log \\lambda_{i}~;~~  b(\\theta_{i}) = \\lambda_{i} = e^{\\theta_{i}}$\n",
        "\n",
        "$b^{'}(\\theta{i})  =   b^{''}(\\theta_{i})  = e^{\\theta_{i}}$\n",
        "\n",
        "$\\Rightarrow b^{'}(\\theta_{i}) = b^{''}(\\theta_{i}) =   \\lambda_{i}$\n",
        "\n",
        "$a(\\phi) =$ 1   \n",
        "\n",
        "\n",
        "#### Remark\n",
        "\n",
        "These two examples illustrate canonical link functions; logit link for Bernoulli and log link for Poisson\n",
        "\n",
        "## <font color = darkgreen>Parameter Estimation  \n",
        "\n",
        "Consider the PDF of an exponential form of distribution\n",
        "\n",
        "$f(y|\\theta,\\phi) = \\exp(\\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y,\\phi))$\n",
        "\n",
        "The log likelihood is\n",
        "\n",
        "$l =  \\sum^{n}_{i=1} (\\frac{y_{i}~\\theta_{i} - b(\\theta_{i})}{a(\\phi_{i})} + c(y_{i},\\phi_{i}))$\n",
        "\n",
        "Then the $i^{th}$ component of the likelihood is \n",
        "$l_i =  \\frac{y_i~\\theta_{i} - b(\\theta_{i})}{a(\\phi_{i})} + c(y_{i},\\phi_{i})$\n",
        "  \n",
        "$\\frac{\\partial l_{i}}{\\partial \\theta_{i}} = \\frac{y_{i}~-~ b^{'}(\\theta_i)}{a(\\phi)}$  \n",
        "\n",
        "$\\Rightarrow \\frac{\\partial^{2} l}{\\partial \\theta_{i}^{2}}  = - \\frac{b^{''}(\\theta)}{a(\\phi)}$ is independent  of  $y_{i}$\n",
        "\n",
        "$\\Rightarrow E[\\frac{\\partial^{2} l}{\\partial \\theta^{2} }] = - \\frac{b^{''}(\\theta)}{a(\\phi)}$ \n",
        "\n",
        "Hence, \n",
        "\n",
        "$\\frac{b^{''}(\\theta)}{a(\\phi)} = \\frac{V(y_{i})}{a(\\phi)^2}$\n",
        "\n",
        "$V(y_{i})  = a(\\phi) b^{''}(\\theta)$\n",
        " \n",
        "Now using  chain relation  \n",
        " \n",
        " \n",
        "$\\frac{\\partial l_{i}}{\\partial \\beta_{j}}   = \\frac{\\partial l_{i}}{\\partial \\theta_{i}}  \\frac{\\partial \\theta_{i}}{\\partial \\mu_{i}} \\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}\\frac{\\partial \\eta_{i}}{\\partial \\beta_{j}}$\n",
        " \n",
        "$\\mu_{i} = b'(\\theta_{i})$\n",
        " \n",
        "i.e ,$\\frac{\\partial \\theta_{i}}{\\partial \\mu_{i}} = \\frac{1}{b''(\\theta_{i})}$\n",
        "\n",
        "Also $\\eta_{i} = \\sum^{k}_{j=0}x_{ij}\\beta_{j}$\n",
        " \n",
        "$\\frac{\\partial \\eta_{i}}{\\partial \\beta_{j}} = x_{ij}$\n",
        " \n",
        "  $\\Rightarrow\\frac{\\partial l_{i}}{\\partial \\beta_{j}} =  \\frac{y_{i} - \\mu_{i}}{a(\\phi)}\\frac{1}{b''(\\theta_{i})}\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}x_{ij}$\n",
        "\n",
        "=$\\frac{y_{i} - \\mu_{i}}{a(\\phi)}\\frac{a(\\phi)}{V(y_{i})}\\frac{\\partial \\mu_{i}}{\\partial \\eta_{j}}x_{ij}$\n",
        "\n",
        "$\\frac{\\partial l_{i}}{\\partial \\beta_{j}} =  \\frac{y_{i} - \\mu_{i}}{V(y_{i})}\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}x_{ij}$\n",
        "\n",
        "i.e \n",
        "$\\frac{\\partial l}{\\partial \\beta} = \\sum^{n}_{i=1} \\frac{y_{i} - \\mu_{i}}{V(y_{i})}\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}x_{ij}$ \n",
        " \n",
        " The term in the sum is $j^{th}$ element of the matrix $X^{T}DV^{-1}(Y- \\mu)$, where D is $\\text{Diag}(\\frac{\\partial\\mu_i}{\\partial\\eta_i})$ and  V is $\\text{Diag}(V(y_{i}))$\n",
        " \n",
        "      \n",
        "Now, $E[\\frac{\\partial^{2} l_{i}}{\\partial \\beta_{h}\\partial \\beta_{j}}]$ \n",
        "\n",
        "$=~E[\\frac{\\partial^{2} l_{i}}{\\partial \\beta_{h}}\\frac{\\partial^{2} l_{i}}{\\partial \\beta_{j}}]$\n",
        "\n",
        "$=~E[\\frac{y_{i} - \\mu_{i}}{V(y_{i})}\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}x_{ih} \\frac{y_{i} - \\mu_{i}}{V(y_{i})}\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}}x_{ij} ]$\n",
        "\n",
        "$=~ \\frac{x_{ih}x_{ij}}{V(y_{i})} (\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i} })^{2}$\n",
        "\n",
        "i.e\n",
        "$E[\\frac{\\partial^{2} l_{i}}{\\partial \\beta_{h}\\partial \\beta_{j} }] = \\sum^{n}_{i=1} \\frac{x_{ih}x_{ij}}{V(Y_{i})} (\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i} })^{2}$\n",
        "\n",
        "This can be written in matrix form as $X_{P \\times n}^{T} W_{n \\times n} X_{n \\times p}$ where $W = \\text{Diag} ( \\frac{(\\frac{\\partial \\mu_{i}}{\\partial \\eta_{i}})^2}{V(y_{i})})$\n",
        "\n",
        "\n",
        "\n",
        "In Exponential Family of distribution, other than normal distribution a closed form solution is not available to solve p equations arising from equating $\\frac{\\partial l}{\\partial\\beta}=0$. Instead, to obtain the maximum likelihood estimator numerically, we must resort to an iterative algorithm such as Newton-Raphson or Fisher scoring method.\n",
        "\n",
        "\n",
        "That is, at the iteration step m,   $\\beta^{(m+1)} = \\beta^{(m)}-[H^{-1}]^{(m)}q^{(m)}$ where H is the Hessian  Matrix. The starting value (m = 0) is assumed to be known \n",
        "\n",
        "$H = [\\frac{\\partial^{2}l_{i}}{\\partial\\beta_{h}\\partial\\beta_{j}}]_{p \\times p}$  and $q = [\\frac{\\partial l}{\\partial \\beta}]_{p \\times 1}$\n",
        "\n",
        "\n",
        "In many cases instead of the observed information matrix [-H] , we can use Fisher expected information matrix  F=E[-H]. Hence, the iterative formula becomes\n",
        "\n",
        "\n",
        "$\\beta^{(m+1)} = \\beta^{(m)} + [F^{-1}]^{(m)}q^{(m)}$\n",
        " \n",
        "The above procedure can be written as a step-by-step procedure\n",
        "\n",
        "### <font color = darkgreen> EXAMPLE 1: Bernoulli Distribution\n",
        "\n",
        "$\\theta_i = \\text{logit}(p_i)$; $b^{'}(\\theta_i)= V(y_i)$\n",
        "\n",
        "$W=\\text{Diag}\\Big(V(y_i)\\Big)_{n\\times n}$\n",
        "\n",
        "The algorithm is, \n",
        "\n",
        "1. Set $\\beta_{p \\times 1} = \\beta_{0}$;\n",
        "\n",
        "1. Find $X_{n \\times p} \\beta_{p \\times 1}$; $\\exp(X\\beta)$ and $1+\\exp(X\\beta)$\n",
        "  \n",
        "1. Find $p_i = \\frac{e^{X\\beta}}{1+e^{X\\beta}}$\n",
        "  \n",
        "1. Find $u_{p \\times 1} = X_{p \\times n }^{T}(Y- \\mu)_{n \\times 1}$ and $W_{n \\times n} =  \\text{Diag}(p_{i}(1-p_{i}))$\n",
        "  \n",
        "1. Find $F_{p  \\times p} = X_{p \\times n }^{T} W_{n \\times n}X_{n \\times p}$ and \n",
        "$F^{-1}$\n",
        "  \n",
        "Hence the iterative equation becomes $\\beta_{p \\times 1}^{(m+1)} = \\beta_{p \\times 1}^{(m)} - [F^{-1}]_{p \\times p}^{(m)}u_{p \\times 1}^{(m)}$\n",
        "\n",
        "  \n",
        "### <font color = darkgreen> EXAMPLE 2: Poisson Distribution\n",
        "\n",
        "In  the case of Poisson distribution,$\\theta_{i} = \\log(\\lambda_{i}) = \\log(\\mu_{i})= \\sum\\beta_{j}x_{ij}$\n",
        "\n",
        "Poisson model with log link\n",
        "\n",
        "$\\theta_{i} = \\log(\\lambda_{i}), \\mu_{i} = b^{'}(\\theta_{i})  =   \\lambda_{i}$ $a(\\phi)=1$\n",
        "\n",
        "$\\Rightarrow V(y_{i}) = b^{''}(\\theta_{i})a(\\phi) = b^{''}(\\theta_{i}) = \\lambda_{i}$\n",
        "\n",
        "$u  =  X^{T}DV^{-1}(Y- \\mu) = X^{T}(Y- \\mu)$\n",
        " \n",
        " \n",
        "$W = \\text{Diag}(\\frac{(b^{''}(\\theta_{i}))^2}{V(y_{i})})  = \\text{Diag}(V(y_{i})) = \\text{Diag}(\\lambda_{i})$\n",
        "\n",
        "**The algorithm is,** \n",
        "\n",
        "1. Set $\\beta_{p \\times 1} = \\beta_{0}$;\n",
        "\n",
        "1. Find $X_{n \\times p} \\beta_{p \\times 1}$; \n",
        "  \n",
        "1. Find $\\lambda_{n\\times 1 } = e^{X\\beta}$\n",
        "  \n",
        "1. Find $u_{p \\times 1} = X_{p \\times n }^{T}(Y- \\mu)_{n \\times 1}$ and $W_{n \\times n} =  \\text{Diag}(\\lambda_{i})$\n",
        "  \n",
        "1. Find $F_{p  \\times p} = X_{p \\times n }^{T} W_{n \\times n}X_{n \\times p}$ and \n",
        "$F^{-1}_{p  \\times p}$\n",
        "  \n",
        "Hence the iterative equation becomes $\\beta_{p \\times 1}^{(m+1)} = \\beta_{p \\times 1}^{(m)} - [F^{-1}]_{p \\times p}^{(m)}u_{p \\times 1}^{(m)}$\n",
        "\n",
        "\n",
        "## <font color = darkgreen>Proof of Preliminary Results\n",
        "\n",
        "### <font color = darkblue>Result 1:$E[\\frac{\\partial L}{\\partial \\theta }] = 0$\n",
        "  \n",
        "**Proof:**\n",
        "\n",
        "Consider  l = $\\ln(f(x|\\theta))$\n",
        "  \n",
        "$E[\\frac{\\partial l}{\\partial \\theta }] =\\frac{f(x|\\theta)} {f^{'}(x|\\theta)}$\n",
        "     \n",
        "$E[\\frac{\\partial l}{\\partial \\theta }] =  \\int \\frac{f(x|\\theta)} {f^{'}(x|\\theta)} f(x|\\theta) \\,dx = \\int \\frac{\\partial }{\\partial \\theta} f(x|\\theta) \\,dx =  \\frac{\\partial }{\\partial \\theta} \\int f(x|\\theta) \\,dx$\n",
        "     \n",
        "$\\Rightarrow E[\\frac{\\partial l}{\\partial \\theta }] = 0$\n",
        "\n",
        "### <font color = darkblue> Result 2:  $E[\\frac{\\partial^{2} l}{\\partial \\theta^{2} }] = -E[(\\frac{\\partial l}{\\partial \\theta })^{2}]$\n",
        "\n",
        "**Proof:**\n",
        "\n",
        "Now ,$E[\\frac{\\partial^{2} l}{\\partial \\theta^{2} }] = \\frac{\\partial }{\\partial \\theta} [\\frac{\\partial l}{\\partial \\theta } ]$\n",
        "\n",
        "\n",
        "$E[\\frac{\\partial^{2} l}{\\partial \\theta^{2} }] =  \\int \\frac{\\partial }{\\partial \\theta}\\frac{\\partial l}{\\partial \\theta }f(x|\\theta) \\,dx$\n",
        "\n",
        "Also from \n",
        "\n",
        "$E[\\frac{\\partial l}{\\partial \\theta }] =  0$\n",
        "\n",
        "$\\Rightarrow$ $\\frac{\\partial }{\\partial \\theta} E[\\frac{\\partial l}{\\partial \\theta }] =  0$\n",
        "\n",
        "$\\Rightarrow$ $\\frac{\\partial }{\\partial \\theta} \\int \\frac{\\partial }{\\partial \\theta} f(x|\\theta) \\,dx= 0$\n",
        "\n",
        "$\\Rightarrow$ $\\int \\frac{\\partial }{\\partial \\theta}\\frac{\\partial l}{\\partial \\theta } f(x|\\theta) \\,dx= 0$\n",
        "\n",
        "$\\Rightarrow$ $\\int \\frac{\\partial^{2} l}{\\partial \\theta^{2} } f(x|\\theta) + \\frac{\\partial }{\\partial \\theta}\\frac{\\partial l}{\\partial \\theta } f(x|\\theta) \\ dx =0$\n",
        " \n",
        " \n",
        " \n",
        "$\\Rightarrow$$E[\\frac{\\partial^{2} l}{\\partial \\theta^{2} }]  +   \\int \\frac{\\partial }{\\partial \\theta} \\frac{\\frac{\\partial }{\\partial \\theta} f(x|\\theta)}{f(x|\\theta)} f(x|\\theta) dx = 0$\n",
        "\n",
        "$E[\\frac{\\partial^{2} l}{\\partial \\theta^{2} }]  +   \\int \\frac{\\partial }{\\partial \\theta} \\frac{\\partial }{\\partial \\theta} f(x|\\theta)\\ dx =  0$\n",
        " \n",
        "$E[\\frac{\\partial^{2} l}{\\partial \\theta^{2} }]  +   E[(\\frac{\\partial l}{\\partial \\theta })^{2}] = 0$\n",
        "  \n",
        "$\\Rightarrow E[\\frac{\\partial^{2} l}{\\partial \\theta^{2} }]  = -    E[(\\frac{\\partial l}{\\partial \\theta })^{2}]$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KPYYKlBI5sPL"
      }
    }
  ]
}